#!/usr/bin/env python3
"""
æµ‹è¯•æ”¹è¿›æ•ˆæœè„šæœ¬
"""

import os
import sys
from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline, set_seed

def test_generation_quality():
    """æµ‹è¯•ç”Ÿæˆè´¨é‡"""
    
    # æµ‹è¯•æç¤ºè¯
    test_prompts = [
        "å•å¸ƒ",
        "é’å…‰é—ªåŠ¨",
        "ä¹”å³°",
        "éƒ­é–å’Œé»„è“‰",
        "åœ¨æ­¦ä¾ ä¸–ç•Œä¸­",
        "å‰‘æ°”çºµæ¨ª"
    ]
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    model_paths = [
        "./enhanced_chinese_model",
        "./chinese_novel_model",
        "./output"
    ]
    
    model_path = None
    for path in model_paths:
        if os.path.exists(path) and os.path.exists(os.path.join(path, "config.json")):
            model_path = path
            break
    
    if not model_path:
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬:")
        print("  python train_small_model.py --mode train")
        print("æˆ–")
        print("  python enhance_training.py")
        return
    
    print(f"âœ… ä½¿ç”¨æ¨¡å‹: {model_path}")
    
    try:
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        tokenizer = GPT2Tokenizer.from_pretrained(model_path)
        model = GPT2LMHeadModel.from_pretrained(model_path)
        
        # åˆ›å»ºç”Ÿæˆå™¨
        generator = pipeline(
            'text-generation',
            model=model,
            tokenizer=tokenizer,
            device=-1
        )
        
        print("\n" + "="*50)
        print("å¼€å§‹æµ‹è¯•æ–‡æœ¬ç”Ÿæˆè´¨é‡")
        print("="*50)
        
        for prompt in test_prompts:
            print(f"\nğŸ“ æç¤º: {prompt}")
            print("-" * 30)
            
            try:
                # ä½¿ç”¨ä¼˜åŒ–çš„å‚æ•°ç”Ÿæˆ
                outputs = generator(
                    prompt,
                    max_new_tokens=50,
                    do_sample=True,
                    temperature=0.8,
                    top_k=50,
                    top_p=0.9,
                    repetition_penalty=1.2,
                    no_repeat_ngram_size=3,
                    num_return_sequences=3,
                    pad_token_id=tokenizer.eos_token_id
                )
                
                for i, output in enumerate(outputs, 1):
                    text = output['generated_text']
                    # è®¡ç®—ç”Ÿæˆé•¿åº¦
                    generated_part = text[len(prompt):].strip()
                    char_count = len(generated_part)
                    word_count = len(generated_part.split())
                    
                    print(f"  ç”Ÿæˆ{i}: {text}")
                    print(f"  æ–°å¢å­—ç¬¦: {char_count}, æ–°å¢è¯æ±‡: {word_count}")
                    
                    # è´¨é‡è¯„ä¼°
                    if char_count > 10:
                        print("  âœ… ç”Ÿæˆè´¨é‡: è‰¯å¥½")
                    elif char_count > 5:
                        print("  âš ï¸  ç”Ÿæˆè´¨é‡: ä¸€èˆ¬")
                    else:
                        print("  âŒ ç”Ÿæˆè´¨é‡: è¾ƒå·®")
                    print()
                    
            except Exception as e:
                print(f"  âŒ ç”Ÿæˆå¤±è´¥: {e}")
                print()
    
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´")

def diagnose_issues():
    """è¯Šæ–­å¸¸è§é—®é¢˜"""
    
    print("\n" + "="*50)
    print("è¯Šæ–­æŠ¥å‘Š")
    print("="*50)
    
    # æ£€æŸ¥è®­ç»ƒæ•°æ®
    text_files = [
        "text/sanguoyanyi.txt",
        "text/tian_long_ba_bu_all.txt",
        "text/romeo_and_juliet.txt"
    ]
    
    total_lines = 0
    for file_path in text_files:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = len(f.readlines())
                total_lines += lines
                print(f"ğŸ“Š {file_path}: {lines} è¡Œ")
        else:
            print(f"âŒ {file_path}: æ–‡ä»¶ä¸å­˜åœ¨")
    
    print(f"ğŸ“Š æ€»æ–‡æœ¬è¡Œæ•°: {total_lines}")
    
    if total_lines < 50000:
        print("âš ï¸  è­¦å‘Š: è®­ç»ƒæ•°æ®é‡å¯èƒ½ä¸è¶³")
        print("ğŸ’¡ å»ºè®®: å¢åŠ æ›´å¤šæ–‡æœ¬æ–‡ä»¶")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_files = [
        "chinese_novel_model/config.json",
        "chinese_novel_model/model.safetensors",
        "output/config.json"
    ]
    
    for file_path in model_files:
        if os.path.exists(file_path):
            size = os.path.getsize(file_path) / (1024*1024)  # MB
            print(f"ğŸ“¦ {file_path}: {size:.1f} MB")
        else:
            print(f"âŒ {file_path}: ä¸å­˜åœ¨")
    
    print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
    print("1. å¢åŠ è®­ç»ƒæ•°æ®é‡ (å»ºè®®100MB+)")
    print("2. å¢åŠ è®­ç»ƒè½®æ¬¡ (å»ºè®®50-100è½®)")
    print("3. ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹æ¶æ„")
    print("4. ä¼˜åŒ–æ–‡æœ¬é¢„å¤„ç†")
    print("5. è°ƒæ•´ç”Ÿæˆå‚æ•°")

if __name__ == "__main__":
    print("ğŸš€ æ–‡æœ¬ç”Ÿæˆè´¨é‡æµ‹è¯•å·¥å…·")
    print("="*50)
    
    if len(sys.argv) > 1 and sys.argv[1] == "diagnose":
        diagnose_issues()
    else:
        test_generation_quality()